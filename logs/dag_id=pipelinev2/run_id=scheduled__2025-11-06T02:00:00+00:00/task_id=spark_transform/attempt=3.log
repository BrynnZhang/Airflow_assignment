{"timestamp":"2025-11-06T02:11:26.312215Z","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager","filename":"manager.py","lineno":179}
{"timestamp":"2025-11-06T02:11:26.312902Z","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/pipeline.py","logger":"airflow.models.dagbag.DagBag","filename":"dagbag.py","lineno":593}
{"timestamp":"2025-11-06T02:11:27.022939Z","level":"info","event":"generated new fontManager","logger":"matplotlib.font_manager","filename":"font_manager.py","lineno":1639}
{"timestamp":"2025-11-06T02:11:27.258112Z","level":"warning","event":"The `airflow.decorators.task` attribute is deprecated. Please use `'airflow.sdk.task'`.","category":"DeprecatedImportWarning","filename":"/opt/airflow/dags/pipeline.py","lineno":8,"logger":"py.warnings"}
{"timestamp":"2025-11-06T02:11:29.447267Z","level":"error","event":"Setting default log level to \"WARN\".","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:29.447768Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:29.750277Z","level":"error","event":"25/11/06 02:11:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:30.962791Z","level":"error","event":"25/11/06 02:11:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.176326Z","level":"error","event":"25/11/06 02:11:40 ERROR FileFormatWriter: Aborting job f9ed7179-3b00-40b3-a97e-c3c56f84a44a.","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.177606Z","level":"error","event":"java.io.FileNotFoundException: File file:/opt/airflow/data/companies_cleaned.csv/_temporary/0/task_202511060211396615805875055362908_0009_m_000000 does not exist","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.178091Z","level":"error","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:597)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.178568Z","level":"error","event":"\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.178988Z","level":"error","event":"\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.180239Z","level":"error","event":"\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.180825Z","level":"error","event":"\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.186141Z","level":"error","event":"\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.186829Z","level":"error","event":"\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.187341Z","level":"error","event":"\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.187563Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.188341Z","level":"error","event":"\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.189189Z","level":"error","event":"\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.189397Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.190288Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.190782Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.194567Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.198160Z","level":"error","event":"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.199530Z","level":"error","event":"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.199945Z","level":"error","event":"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.200881Z","level":"error","event":"\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.201130Z","level":"error","event":"\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.201336Z","level":"error","event":"\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.201480Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.201782Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.202109Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.202849Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.203099Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.203806Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.204182Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.204501Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.204740Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.205102Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.205425Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.206047Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.206688Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.207549Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.207885Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.208618Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.210154Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.210603Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.210935Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.211325Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.211581Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.211777Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.212206Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.212750Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.213591Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.213956Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.214190Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.214322Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.214460Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.214566Z","level":"error","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.214684Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.214779Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.215028Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.216362Z","level":"error","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.216618Z","level":"error","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.216800Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.217254Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.217505Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","logger":"task.stderr"}
{"timestamp":"2025-11-06T02:11:40.219440Z","level":"error","event":"Task failed with exception","logger":"task","filename":"task_runner.py","lineno":994,"error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o56.csv.\n: java.io.FileNotFoundException: File file:/opt/airflow/data/companies_cleaned.csv/_temporary/0/task_202511060211396615805875055362908_0009_m_000000 does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:597)\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":920,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1307,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":416,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/decorator.py","lineno":252,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":416,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":216,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":239,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":82,"name":"run"},{"filename":"/opt/airflow/dags/pipeline.py","lineno":109,"name":"spark_transform"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":1864,"name":"csv"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}],"is_group":false,"exceptions":[]}]}
